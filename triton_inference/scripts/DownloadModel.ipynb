{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41192dad",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df0dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/russian_toxicity_classifier\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"s-nlp/russian_toxicity_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c3928",
   "metadata": {},
   "source": [
    "### I did the installing part partly here, partly with conda. Holy shit it's so slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24713444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnx in c:\\users\\ulyan\\appdata\\roaming\\python\\python311\\site-packages (1.17.0)\n",
      "Collecting onnxconverter-common\n",
      "  Obtaining dependency information for onnxconverter-common from https://files.pythonhosted.org/packages/6d/6a/9ed9fd4da34cb41fda35bc5cc9e990c605689db7de63ed84fedbca5a77f6/onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\e\\downloads\\anaconda\\lib\\site-packages (from onnx) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\ulyan\\appdata\\roaming\\python\\python311\\site-packages (from onnx) (5.29.3)\n",
      "Requirement already satisfied: packaging in d:\\e\\downloads\\anaconda\\lib\\site-packages (from onnxconverter-common) (23.1)\n",
      "Collecting protobuf>=3.20.2 (from onnx)\n",
      "  Obtaining dependency information for protobuf>=3.20.2 from https://files.pythonhosted.org/packages/8b/e6/2a47ce2eba1aaf287380a44270da897ada03d118a55c19595ec7b4f0831f/protobuf-3.20.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading protobuf-3.20.2-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/84.5 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/84.5 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/84.5 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 41.0/84.5 kB 245.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 84.5/84.5 kB 431.6 kB/s eta 0:00:00\n",
      "Downloading protobuf-3.20.2-py2.py3-none-any.whl (162 kB)\n",
      "   ---------------------------------------- 0.0/162.1 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 112.6/162.1 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 162.1/162.1 kB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, onnxconverter-common\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.3\n",
      "    Uninstalling protobuf-5.29.3:\n",
      "      Successfully uninstalled protobuf-5.29.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Отказано в доступе: 'C:\\\\Users\\\\ulyan\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\google\\\\~upb\\\\_message.pyd'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install onnx onnxconverter-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b1bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import os\n",
    "\n",
    "PREFIX_MODELS = \"triton_inference//model_repository\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07a4d5",
   "metadata": {},
   "source": [
    "### Saving models \n",
    "- pth (original)\n",
    "- onnx_fp32\n",
    "- onnx_fp16\n",
    "- trt_fp16\n",
    "\n",
    "Actually you might notice that I'm saving in simply folders, and I had to manually move all thiose files to 1/ folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1190e274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('triton_inference//model_repository//pth\\\\tokenizer_config.json',\n",
       " 'triton_inference//model_repository//pth\\\\special_tokens_map.json',\n",
       " 'triton_inference//model_repository//pth\\\\vocab.txt',\n",
       " 'triton_inference//model_repository//pth\\\\added_tokens.json',\n",
       " 'triton_inference//model_repository//pth\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# orig\n",
    "model.save_pretrained(f\"{PREFIX_MODELS}//pth\")\n",
    "tokenizer.save_pretrained(f\"{PREFIX_MODELS}//pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb7a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx_fp32\n",
    "onnx_path = f\"{PREFIX_MODELS}/onnx_fp32/model.onnx\"\n",
    "\n",
    "sequence_length = 128\n",
    "dummy_input = tokenizer(\"Это пример текста.\", padding='max_length', truncation=True, max_length=sequence_length, return_tensors=\"pt\")\n",
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "output_names = [\"output\"]\n",
    "dynamic_axes = {\n",
    "    \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "    \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "    \"output\": {0: \"batch_size\"}\n",
    "}\n",
    "\n",
    "model.eval()\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
    "    onnx_path,\n",
    "    opset_version=13,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    do_constant_folding=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d6306b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 3.5592728764299864e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -8.079577284991757e-11 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 4.6042714529903606e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 6.83335201756563e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.2669810917363975e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -7.162196169474555e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -5.64563009675112e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 7.834561444042265e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.299947882671404e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.2253814790929027e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.691666611820409e-10 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -5.6806953807608807e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 5.344382003613646e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -3.8871029062192974e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 6.015055475927511e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 6.299678112497986e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.3603093762526441e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 5.008268999517895e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -8.689892894153672e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 5.371569855583402e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -5.524969282078018e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.7463496365953688e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -2.5352724364324786e-09 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.716609159847394e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.5091488947405196e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -6.349973880048765e-09 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -2.7797153023811916e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -4.939952802374137e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 1.053004972106919e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.2360302115155264e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.2574504043859633e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -9.114307175650538e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 9.080340390710262e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 5.124388025024018e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.296058727007221e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 1.1694718971000384e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -5.880517051082279e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 1.047466557935195e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -8.949656016277174e-10 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -4.765707828369159e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -9.016616786539089e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -7.011335867446178e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.700387469189991e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -6.760819104556504e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 6.82364520443457e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.1055823751225944e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -4.8324391599408045e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 3.9351345293425766e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.151856026699761e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 8.810991047880634e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.8684556124526353e-09 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 3.11328633983976e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -5.1098766107315896e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 7.732945839222793e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -3.925245906089003e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -2.6041382383823475e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 7.242814348273896e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.808357552590678e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.8909674654187256e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 7.653825662146119e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.124302511674614e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -3.1940491140858285e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -4.470800174516398e-09 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 1.6986591688805674e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 1.660020920724037e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.987756531069863e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -9.155943558880608e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 6.032205401851343e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.2621200085050077e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 3.7797423857455215e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -2.1835933949887476e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 3.887680932734838e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 5.289872362368442e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -6.286138898303761e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -5.802453184600154e-09 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.4465307174305053e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 3.864899511540898e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -2.6489113125194308e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 3.783402036106054e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -2.67530353426082e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 9.911589415878552e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -7.809173041550821e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -8.446092181202403e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 5.5519691954941663e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.2897726442417934e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 1.2124660386803043e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -3.320745989299212e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 1.1888504403145816e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -2.4564103640045687e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -8.313823229855188e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 8.719081279195962e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -4.711100842769156e-09 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 3.039138718463619e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -4.208645432157709e-09 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.026594003723403e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.533186022584232e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -2.1019026519297768e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.430185830348819e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 6.186648704442632e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -8.396430217771922e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 8.250463645609329e-10 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -3.196081976852838e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 2.5217516963493836e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -1.1073137073935868e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 7.47634398834407e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:53: UserWarning: the float32 number -8.269574891528464e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:50: UserWarning: the float32 number -3.4028234663852886e+38 will be truncated to -10000.0\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_min, -max_finite_val))\n",
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\onnxconverter_common\\float16.py:43: UserWarning: the float32 number 9.999999960041972e-13 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n"
     ]
    }
   ],
   "source": [
    "from onnxconverter_common import float16\n",
    "\n",
    "onnx_fp32_path = f\"{PREFIX_MODELS}/onnx_fp32/model.onnx\"\n",
    "onnx_fp16_path = f\"{PREFIX_MODELS}/onnx_fp16/model.onnx\"\n",
    "\n",
    "onnx_model_fp16 = float16.convert_float_to_float16(onnx_model)\n",
    "onnx.save(onnx_model_fp16, onnx_fp16_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c63ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import os\n",
    "\n",
    "TRT_LOGGER = trt.Logger()\n",
    "\n",
    "def build_engine(engine_file_path, fp16_mode=True):\n",
    "    with (trt.Builder(TRT_LOGGER) as builder, \n",
    "          builder.create_builder_config() as config, \n",
    "          builder.create_network(1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network,\n",
    "          trt.OnnxParser(network, TRT_LOGGER) as parser):\n",
    "        config.max_workspace_size = 1 << 30  # 1G\n",
    "        if fp16_model:\n",
    "            config.flags |= 1 << int(trt.BuilderFlag.FP16)\n",
    "\n",
    "        engine = builder.build_serialized_network(network, config)\n",
    "        if engine is None:\n",
    "            print(\"ERROR: Could not build engine.\")\n",
    "            return None\n",
    "\n",
    "        with open(engine_file_path, \"wb\") as f:\n",
    "            f.write(engine)\n",
    "        return engine\n",
    "\n",
    "trt_engine_path = f\"{PREFIX_MODELS}/trt_fp16/model.plan\"\n",
    "\n",
    "engine = build_engine(onnx_fp16_path, trt_engine_path)\n",
    "if engine:\n",
    "    print(f\"TensorRT engine saved to {trt_engine_path}\")\n",
    "else:\n",
    "    print(\"TensorRT engine build failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6dc4a",
   "metadata": {},
   "source": [
    "Then I manually made configs and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47897f87",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "I am working on windows so no gpu in triton sorry\n",
    "Also as one of the comments claimed - I used a [different dataset](https://huggingface.co/datasets/AlexSham/Toxic_Russian_Comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0953c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "splits = {'train': 'train.jsonl', 'test': 'test.jsonl'}\n",
    "df_train = pd.read_json(\"hf://datasets/AlexSham/Toxic_Russian_Comments/\" + splits[\"train\"], lines=True)\n",
    "df_test = pd.read_json(\"hf://datasets/AlexSham/Toxic_Russian_Comments/\" + splits[\"test\"], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07750e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>видимо в разных регионах называют по разному ,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>понятно что это нарушение правил, писать капсл...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>какие классные, жизненные стихи....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>а и правда-когда его запретили?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>в соленой воде вирусы живут .ученые изучали со...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  видимо в разных регионах называют по разному ,...      0\n",
       "1  понятно что это нарушение правил, писать капсл...      1\n",
       "2                какие классные, жизненные стихи....      0\n",
       "3                 а и правда-когда его запретили?...      0\n",
       "4  в соленой воде вирусы живут .ученые изучали со...      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03c952f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce296d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train...:   0%|                                                         | 8/6984 [00:45<10:56:17,  5.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     11\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     12\u001b[0m all_logits_list\u001b[38;5;241m.\u001b[39mappend(logits)\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[0;32m   1563\u001b[0m     input_ids,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1565\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1566\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1567\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1568\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1569\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1570\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1571\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1572\u001b[0m )\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    613\u001b[0m         hidden_states,\n\u001b[0;32m    614\u001b[0m         attention_mask,\n\u001b[0;32m    615\u001b[0m         layer_head_mask,\n\u001b[0;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    618\u001b[0m         past_key_value,\n\u001b[0;32m    619\u001b[0m         output_attentions,\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    541\u001b[0m )\n\u001b[0;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\transformers\\pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    465\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\E\\Downloads\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_logits_list = []\n",
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0, len(df_train), batch_size), desc=\"Processing train...\"):\n",
    "    batch_texts = df_train.loc[i:min(i + batch_size, len(df_train)), \"text\"].to_list()\n",
    "    inputs = tokenizer(batch_texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.cpu().numpy()\n",
    "    all_logits_list.append(logits)\n",
    "\n",
    "all_logits = np.concatenate(all_logits_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b377c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = np.exp(all_logits[:, 1]) / np.sum(np.exp(all_logits), axis=1)\n",
    "\n",
    "def find_optimal_threshold(probabilities, labels, target_precision=0.9):\n",
    "    thresholds = np.sort(probabilities)\n",
    "    for threshold in thresholds:\n",
    "        predictions = (probabilities >= threshold).astype(int)\n",
    "        print(len(labels), len(predictions))\n",
    "        precision = precision_score(labels, predictions)\n",
    "        if precision >= target_precision:\n",
    "            return threshold\n",
    "    return 0.5\n",
    "\n",
    "optimal_threshold = find_optimal_threshold(probabilities, df_train[\"label\"], target_precision=0.9)\n",
    "print(f\"Optimal threshold with 0.9 precision: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(df_train[\"label\"], probabilities)\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, probabilities)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2deef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_idx = np.argmin(np.abs(thresholds - optimal_threshold))\n",
    "plt.scatter(fpr[closest_idx], tpr[closest_idx], color='red', label=f'Threshold = {optimal_threshold:.2f}')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(test_labels, probabilities)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'PR curve (area = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "recall_threshold_index = np.argmin(np.abs(thresholds - optimal_threshold))\n",
    "\n",
    "plt.scatter(recall[recall_threshold_index], precision[recall_threshold_index], color='red', label=f'Threshold = {optimal_threshold:.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc2c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
